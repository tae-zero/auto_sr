#!/usr/bin/env python3
"""
RAG ì„ë² ë”© ì„œë¹„ìŠ¤
SR PDFì™€ TCFD ê¸°ì¤€ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥
"""
## pgvector 

import os
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
import pandas as pd
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RAGEmbeddingService:
    def __init__(self, base_path: str = "document"):
        self.base_path = Path(base_path)
        self.chroma_path = Path("chroma_db")
        self.chroma_path.mkdir(exist_ok=True)
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-base",
            model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (ê¶Œì¥ íŒŒë¼ë¯¸í„°)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            length_function=len,
            separators=["\n## ", "\n#", "\n\n", "\n", " "]
        )
        
        logger.info("RAG ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_pdf_documents(self, pdf_dir: str) -> List[Dict[str, Any]]:
        """PDF ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• """
        pdf_path = self.base_path / pdf_dir
        documents = []
        
        if not pdf_path.exists():
            logger.error(f"PDF ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")
            return documents
        
        # TCFD ë§¤í•‘ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        tcfd_mapping = self.load_tcfd_mapping()
        
        for pdf_file in pdf_path.glob("*.pdf"):
            try:
                logger.info(f"PDF ë¡œë”© ì¤‘: {pdf_file.name}")
                loader = PyPDFLoader(str(pdf_file))
                pages = loader.load()
                
                # íŒŒì¼ëª…ì—ì„œ íšŒì‚¬ëª…ê³¼ ì—°ë„ ì¶”ì¶œ
                company, year = self.extract_company_year(pdf_file.name)
                
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ê°œì„ ëœ ë²„ì „)
                for i, page in enumerate(pages):
                    metadata = {
                        "collection": pdf_dir,
                        "source": pdf_file.name,
                        "company": company,
                        "year": year,
                        "page_from": i + 1,
                        "page_to": i + 1,
                        "type": "pdf"
                    }
                    
                    # TCFD ë§¤í•‘ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                    if tcfd_mapping and company in tcfd_mapping:
                        metadata.update(tcfd_mapping[company])
                    
                    page.metadata.update(metadata)
                
                documents.extend(pages)
                logger.info(f"âœ… {pdf_file.name} ë¡œë”© ì™„ë£Œ ({len(pages)} í˜ì´ì§€)")
                
            except Exception as e:
                logger.error(f"âŒ {pdf_file.name} ë¡œë”© ì‹¤íŒ¨: {e}")
        
        return documents
    
    def extract_company_year(self, filename: str) -> tuple:
        """íŒŒì¼ëª…ì—ì„œ íšŒì‚¬ëª…ê³¼ ì—°ë„ ì¶”ì¶œ"""
        # íŒŒì¼ëª… íŒ¨í„´: íšŒì‚¬ëª…_ì—°ë„.pdf
        parts = filename.replace('.pdf', '').split('_')
        
        if len(parts) >= 2:
            company = parts[0]
            year = parts[-1]  # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ì—°ë„
            return company, year
        
        return filename.replace('.pdf', ''), "unknown"
    
    def load_tcfd_mapping(self) -> Dict:
        """TCFD ë§¤í•‘ ì •ë³´ ë¡œë“œ"""
        mapping_path = self.base_path / "tcfd_mapping.xlsx"
        
        if not mapping_path.exists():
            logger.info("TCFD ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            df = pd.read_excel(mapping_path)
            mapping = {}
            
            for _, row in df.iterrows():
                company = row.get('company', '')
                if company:
                    mapping[company] = {
                        "pillar": row.get('pillar', ''),
                        "section": row.get('section', '')
                    }
            
            logger.info(f"TCFD ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(mapping)}ê°œ íšŒì‚¬")
            return mapping
            
        except Exception as e:
            logger.error(f"TCFD ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def load_excel_documents(self, excel_file: str) -> List[Dict[str, Any]]:
        """Excel íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• """
        excel_path = self.base_path / excel_file
        
        if not excel_path.exists():
            logger.error(f"Excel íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {excel_path}")
            return []
        
        try:
            logger.info(f"Excel ë¡œë”© ì¤‘: {excel_file}")
            df = pd.read_excel(excel_path)
            
            documents = []
            for idx, row in df.iterrows():
                # ê° í–‰ì„ ë¬¸ì„œë¡œ ë³€í™˜
                content = ""
                metadata = {"source": excel_file, "type": "excel", "row": idx + 1}
                
                for col in df.columns:
                    if pd.notna(row[col]):
                        content += f"{col}: {row[col]}\n"
                        metadata[col] = str(row[col])
                
                if content.strip():
                    documents.append({
                        "page_content": content.strip(),
                        "metadata": metadata
                    })
            
            logger.info(f"âœ… {excel_file} ë¡œë”© ì™„ë£Œ ({len(documents)} í–‰)")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ {excel_file} ë¡œë”© ì‹¤íŒ¨: {e}")
            return []
    
    def create_vectorstore(self, documents: List[Dict[str, Any]], collection_name: str) -> Chroma:
        """ë¬¸ì„œë“¤ì„ ì„ë² ë”©í•˜ì—¬ ChromaDB ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
        if not documents:
            logger.warning(f"ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: {collection_name}")
            return None
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        logger.info(f"í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘: {collection_name}")
        texts = []
        metadatas = []
        
        for doc in documents:
            if hasattr(doc, 'page_content'):
                # LangChain Document ê°ì²´
                chunks = self.text_splitter.split_text(doc.page_content)
                for chunk in chunks:
                    texts.append(chunk)
                    metadatas.append(doc.metadata)
            else:
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ
                chunks = self.text_splitter.split_text(doc["page_content"])
                for chunk in chunks:
                    texts.append(chunk)
                    metadatas.append(doc["metadata"])
        
        logger.info(f"âœ… {collection_name}: {len(texts)}ê°œ ì²­í¬ ìƒì„±")
        
        # ChromaDB ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorstore = Chroma.from_texts(
            texts=texts,
            embedding=self.embedding_model,
            metadatas=metadatas,
            collection_name=collection_name,
            persist_directory=str(self.chroma_path / collection_name)
        )
        
        # ì €ì¥
        vectorstore.persist()
        logger.info(f"âœ… {collection_name} ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ")
        
        return vectorstore
    
    def create_sr_corpus(self):
        """SR PDF ë¬¸ì„œë“¤ì„ ì„ë² ë”©í•˜ì—¬ sr_corpus ìƒì„±"""
        logger.info("ğŸ”¹ SR ì½”í¼ìŠ¤ ìƒì„± ì‹œì‘")
        
        # SR PDF ë¬¸ì„œ ë¡œë“œ
        documents = self.load_pdf_documents("sr")
        
        if documents:
            # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = self.create_vectorstore(documents, "sr_corpus")
            
            if vectorstore:
                # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                results = vectorstore.similarity_search("ê¸°í›„ë³€í™”", k=3)
                logger.info(f"âœ… SR ì½”í¼ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼")
        
        logger.info("ğŸ”¹ SR ì½”í¼ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def create_tcfd_standards(self):
        """TCFD ê¸°ì¤€ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ standards ìƒì„±"""
        logger.info("ğŸ”¹ TCFD Standards ìƒì„± ì‹œì‘")
        
        # TCFD ê¸°ì¤€ì„œ PDF ë¡œë“œ
        documents = self.load_pdf_documents("tcfd")
        
        if documents:
            # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chroma_db/standards)
            vectorstore = self.create_vectorstore(documents, "standards")
            
            if vectorstore:
                # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                results = vectorstore.similarity_search("ê±°ë²„ë„ŒìŠ¤", k=3)
                logger.info(f"âœ… TCFD Standards ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼")
        
        logger.info("ğŸ”¹ TCFD Standards ìƒì„± ì™„ë£Œ")
    
    def create_all_embeddings(self):
        """ëª¨ë“  ì„ë² ë”© ìƒì„±"""
        logger.info("ğŸš€ ì „ì²´ ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        # SR ì½”í¼ìŠ¤ ìƒì„±
        self.create_sr_corpus()
        
        # TCFD Standards ìƒì„±
        self.create_tcfd_standards()
        
        logger.info("ğŸ‰ ì „ì²´ ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        
        # ì €ì¥ëœ ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        self.print_collection_info()
    
    def print_collection_info(self):
        """ì €ì¥ëœ ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥"""
        logger.info("ğŸ“Š ì €ì¥ëœ ì»¬ë ‰ì…˜ ì •ë³´:")
        
        for collection_dir in self.chroma_path.iterdir():
            if collection_dir.is_dir():
                collection_name = collection_dir.name
                chroma_path = str(collection_dir)
                
                try:
                    # ChromaDB í´ë¼ì´ì–¸íŠ¸ë¡œ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ
                    client = chromadb.PersistentClient(path=chroma_path)
                    collection = client.get_collection(name=collection_name)
                    count = collection.count()
                    
                    logger.info(f"  - {collection_name}: {count}ê°œ ë¬¸ì„œ")
                    
                except Exception as e:
                    logger.warning(f"  - {collection_name}: ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({e})")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    service = RAGEmbeddingService()
    service.create_all_embeddings()

if __name__ == "__main__":
    main()
