#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RAG ì„ë² ë”© ì„œë¹„ìŠ¤ (pgvector ì „ìš©)
- document/sr, document/tcfd ì„ë² ë”© â†’ Postgres(pgvector)ì— ì €ì¥
- Chroma/FAISS ì½”ë“œ ì—†ìŒ
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple
import pandas as pd

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import PGVector
from langchain.document_loaders import PyPDFLoader

# ---------- ê²½ë¡œ/í™˜ê²½ ----------
PROJECT_ROOT = Path(__file__).resolve().parents[1]              # C:\taezero\auto_sr
BASE_PATH = PROJECT_ROOT / "document"
SR_DIR = BASE_PATH / "sr"
TCFD_DIR = BASE_PATH / "tcfd"

PG_CONN_STR = os.getenv("PG_CONN_STR", "")  # ex) postgresql+psycopg://user:pwd@host:5432/db?sslmode=require

# ---------- ë¡œê¹… ----------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("rag_pgvector")

# ---------- ì„ë² ë”© ëª¨ë¸ ----------
EMBED_MODEL_NAME = "intfloat/multilingual-e5-base"
embeddings = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

# ---------- í…ìŠ¤íŠ¸ ë¶„í•  ----------
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    length_function=len,
    separators=["\n## ", "\n#", "\n\n", "\n", " "]
)

# ---------- ìœ í‹¸ ----------
def extract_company_year(filename: str) -> Tuple[str, str]:
    """íŒŒì¼ëª… 'íšŒì‚¬ëª…_ì—°ë„.pdf' í˜•íƒœì—ì„œ íšŒì‚¬/ì—°ë„ ì¶”ì¶œ"""
    name = filename.replace(".pdf", "")
    parts = name.split("_")
    if len(parts) >= 2:
        return parts[0], parts[-1]
    return name, "unknown"

def load_tcfd_mapping() -> Dict[str, Dict[str, str]]:
    """
    TCFD ë§¤í•‘: document/TCFD_MAPPING_FINAL.xlsx ë¥¼ ì½ì–´
    company í‚¤ë¡œ ëŒ€í‘œ ë ˆì½”ë“œë¥¼ 1ê°œì”© ì €ì¥.
    í•„ìš”í•œ í•„ë“œë§Œ ë‚¨ê¸°ê³  ì»¬ëŸ¼ëª… ì •ê·œí™”.
    """
    xls = BASE_PATH / "TCFD_MAPPING_FINAL.xlsx"
    if not xls.exists():
        logger.info("TCFD ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ê±´ë„ˆëœ€)")
        return {}
    try:
        df = pd.read_excel(xls)

        # ì—‘ì…€ì—ì„œ ì˜ë¦° ì»¬ëŸ¼ëª… ë³´ì •
        rename_map = {'...on': 'description'}
        df = df.rename(columns=rename_map)

        # í•„ìš” ì»¬ëŸ¼ í™•ë³´
        needed = ['company', 'year', 'pillar', 'section_tag',
                  'tcfd_section', 'requirement_id', 'page_from', 'page_to']
        for col in needed:
            if col not in df.columns:
                df[col] = None

        # íšŒì‚¬ë³„ ëŒ€í‘œ 1í–‰ (ì—¬ëŸ¬ ê°œë©´ ë§ˆì§€ë§‰)
        df = df.sort_index()
        grouped = df.groupby('company', dropna=True).tail(1)

        mapping: Dict[str, Dict[str, str]] = {}
        for _, r in grouped.iterrows():
            company = str(r.get('company') or '').strip()
            if not company:
                continue
            mapping[company] = {
                'pillar': str(r.get('pillar') or ''),
                'section_tag': str(r.get('section_tag') or ''),
                'tcfd_section': str(r.get('tcfd_section') or ''),
                'requirement_id': str(r.get('requirement_id') or ''),
                'mapped_page_from': str(r.get('page_from') or ''),
                'mapped_page_to': str(r.get('page_to') or ''),
                'mapping_year': str(r.get('year') or ''),
            }
        logger.info(f"TCFD ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(mapping)}ê°œ íšŒì‚¬")
        return mapping
    except Exception as e:
        logger.error(f"TCFD ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def load_pdf_dir(pdf_dir: Path) -> List[Any]:
    """PDF ë””ë ‰í† ë¦¬ë¥¼ í˜ì´ì§€ ë‹¨ìœ„ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜(+ë©”íƒ€)"""
    docs: List[Any] = []
    if not pdf_dir.exists():
        logger.error(f"PDF ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_dir}")
        return docs

    tcfd_map = load_tcfd_mapping()

    for pdf in pdf_dir.glob("*.pdf"):
        try:
            logger.info(f"PDF ë¡œë”© ì¤‘: {pdf.name}")
            pages = PyPDFLoader(str(pdf)).load()
            company, year = extract_company_year(pdf.name)
            for i, page in enumerate(pages):
                meta = {
                    "collection": pdf_dir.name,
                    "source": pdf.name,
                    "company": company,
                    "year": year,
                    "page_from": i + 1,
                    "page_to": i + 1,
                    "type": "pdf",
                }
                # ë§¤í•‘ ì •ë³´ ë³‘í•© (ì¡´ì¬ ì‹œ)
                if company in tcfd_map:
                    meta.update(tcfd_map[company])
                page.metadata.update(meta)
            docs.extend(pages)
            logger.info(f"âœ… {pdf.name} ë¡œë”© ì™„ë£Œ ({len(pages)} í˜ì´ì§€)")
        except Exception as e:
            logger.error(f"âŒ {pdf.name} ë¡œë”© ì‹¤íŒ¨: {e}")
    return docs

def split_docs(docs: List[Any]) -> Tuple[List[str], List[Dict[str, Any]]]:
    """LangChain Document ë¦¬ìŠ¤íŠ¸ë¥¼ ì²­í¬ í…ìŠ¤íŠ¸/ë©”íƒ€ë¡œ ë³€í™˜"""
    texts, metas = [], []
    for d in docs:
        chunks = text_splitter.split_text(d.page_content)
        for c in chunks:
            texts.append(c)
            metas.append(dict(d.metadata))
    return texts, metas

# ---------- ì €ì¥ ----------
def upsert_pgvector(texts: List[str], metadatas: List[Dict[str, Any]], collection_name: str):
    """pgvector(Postgres)ì— ì—…ì„œíŠ¸"""
    if not PG_CONN_STR:
        raise SystemExit("PG_CONN_STR í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”. (pgvector Postgres ì ‘ì† ë¬¸ìì—´)")
    PGVector.from_texts(
        texts=texts,
        embedding=embeddings,
        metadatas=metadatas,
        collection_name=collection_name,
        connection_string=PG_CONN_STR,
    )
    logger.info(f"âœ… pgvector ì—…ì„œíŠ¸ ì™„ë£Œ: collection={collection_name}, chunks={len(texts)}")

def build_collection(pdf_dir: Path, collection_name: str):
    docs = load_pdf_dir(pdf_dir)
    if not docs:
        logger.warning(f"ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: {collection_name}")
        return
    texts, metas = split_docs(docs)
    logger.info(f"âœ… {collection_name}: {len(texts)}ê°œ ì²­í¬ ìƒì„±")
    upsert_pgvector(texts, metas, collection_name)

def main():
    logger.info("ğŸš€ pgvector ì„ë² ë”© ì—…ì„œíŠ¸ ì‹œì‘")
    build_collection(SR_DIR, "sr_corpus")
    build_collection(TCFD_DIR, "standards")
    logger.info("ğŸ‰ ì™„ë£Œ")

if __name__ == "__main__":
    main()
