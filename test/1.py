#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
FAISS ì¸ë±ìŠ¤ ë¹Œë” (ë‹¨ì¼/ë‹¤ì¤‘ PDF)
- ì…ë ¥: PDF ê²½ë¡œ(ë“¤)
- ì¶œë ¥: vectordb/<collection>/index.faiss, index.pkl
- ì„ë² ë”©: intfloat/multilingual-e5-base (ì½”ì‚¬ì¸ ì •ê·œí™”)
- ê²€ìƒ‰íƒ€ì…: COSINE
"""

import os
import logging
from pathlib import Path
from typing import List, Tuple, Dict, Any

import pandas as pd
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS, DistanceStrategy
from langchain_huggingface import HuggingFaceEmbeddings

# ---------- ê²½ë¡œ/í™˜ê²½ ----------
PROJECT_ROOT = Path(__file__).resolve().parent
VECTORDIR = PROJECT_ROOT / "vectordb" / "demo"   # <- ì €ì¥ë  ê²½ë¡œ(ì›í•˜ë©´ ë³€ê²½)
VECTORDIR.mkdir(parents=True, exist_ok=True)

# PDF ì˜ˆì‹œ: ì—…ë¡œë“œ íŒŒì¼ ê²½ë¡œ
# - ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œ: PDF_PATHS = ["./document/sr/ì‚¼ì„±ì „ì_2024.pdf", "./document/tcfd/TCFD.pdf"]
# - ë³¸ ìš”ì²­ íŒŒì¼: /mnt/data/ì •íƒœì˜.pdf
PDF_PATHS = ["ì •íƒœì˜.pdf"]  # í•„ìš”ì‹œ ì—¬ëŸ¬ ê°œ ì¶”ê°€ ê°€ëŠ¥

# ---------- ë¡œê¹… ----------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
log = logging.getLogger("faiss_builder")

# ---------- ì„ë² ë”© ----------
EMBED_MODEL_NAME = os.environ.get("EMBED_MODEL_NAME", "intfloat/multilingual-e5-base")
EMBED_DEVICE = os.environ.get("EMBED_DEVICE", "cuda")  # "cuda" or "cpu"
EMBED_BATCH_SIZE = int(os.environ.get("EMBED_BATCH_SIZE", "16"))

embeddings = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": EMBED_DEVICE},
    encode_kwargs={
        "normalize_embeddings": True,
        "batch_size": EMBED_BATCH_SIZE,
    },
)

# ---------- í…ìŠ¤íŠ¸ ë¶„í•  ----------
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    separators=["\n## ", "\n#", "\n\n", "\n", " "],
)

def extract_company_year(filename: str) -> Tuple[str, str]:
    """íŒŒì¼ëª… 'íšŒì‚¬ëª…_ì—°ë„.pdf' â†’ (íšŒì‚¬ëª…, ì—°ë„) ê°„ë‹¨ ì¶”ì¶œ"""
    base = Path(filename).name.replace(".pdf", "")
    parts = base.split("_")
    if len(parts) >= 2:
        return parts[0], parts[-1]
    return base, "unknown"

def load_pdfs(pdf_paths: List[str]) -> List[Any]:
    """PDFë“¤ì„ LangChain Documentsë¡œ ë¡œë“œ(í˜ì´ì§€ ë‹¨ìœ„), ê¸°ë³¸ ë©”íƒ€ ì¶”ê°€"""
    docs = []
    for p in pdf_paths:
        pth = Path(p)
        if not pth.exists():
            log.warning(f"PDF ì—†ìŒ: {pth}")
            continue
        try:
            log.info(f"PDF ë¡œë”©: {pth.name}")
            pages = PyPDFLoader(str(pth)).load()
            company, year = extract_company_year(pth.name)
            for i, page in enumerate(pages):
                meta = {
                    "source": pth.name,
                    "company": company,
                    "year": year,
                    "page_from": i+1,
                    "page_to": i+1,
                    "type": "pdf",
                    "collection": "demo",
                }
                page.metadata.update(meta)
            docs.extend(pages)
            log.info(f"  -> {pth.name} {len(pages)} pages")
        except Exception as e:
            log.error(f"ë¡œë”© ì‹¤íŒ¨({pth.name}): {e}")
    return docs

def split_docs(docs) -> Tuple[List[str], List[Dict[str, Any]]]:
    texts, metas = [], []
    for d in docs:
        chunks = text_splitter.split_text(d.page_content or "")
        for c in chunks:
            c_strip = c.strip()
            if not c_strip:
                continue
            texts.append(c_strip)
            metas.append(dict(d.metadata))
    return texts, metas

def upsert_faiss(texts: List[str], metas: List[Dict[str, Any]], save_dir: Path):
    save_dir.mkdir(parents=True, exist_ok=True)
    idx_file = save_dir / "index.faiss"
    pkl_file = save_dir / "index.pkl"

    if idx_file.exists() and pkl_file.exists():
        store = FAISS.load_local(
            str(save_dir), embeddings,
            allow_dangerous_deserialization=True,
            distance_strategy=DistanceStrategy.COSINE,
        )
        store.add_texts(texts, metadatas=metas)
        store.save_local(str(save_dir))
        log.info(f"ğŸ” ì¶”ê°€ ì €ì¥: +{len(texts)} chunks â†’ {save_dir}")
    else:
        store = FAISS.from_texts(
            texts,
            embeddings,
            metadatas=metas,
            distance_strategy=DistanceStrategy.COSINE,
        )
        store.save_local(str(save_dir))
        log.info(f"ğŸ†• ì‹ ê·œ ì¸ë±ìŠ¤ ìƒì„±: chunks={len(texts)} â†’ {save_dir}")

def main():
    log.info(f"ğŸš€ FAISS ë¹Œë“œ ì‹œì‘ (device={EMBED_DEVICE}, model={EMBED_MODEL_NAME}, batch={EMBED_BATCH_SIZE})")
    docs = load_pdfs(PDF_PATHS)
    if not docs:
        log.error("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
        return
    texts, metas = split_docs(docs)
    log.info(f"ì´ ì²­í¬: {len(texts)}")
    if not texts:
        log.error("ë¹ˆ ì²­í¬. ì¢…ë£Œ.")
        return
    upsert_faiss(texts, metas, VECTORDIR)

    # ê°„ë‹¨ ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    try:
        store = FAISS.load_local(
            str(VECTORDIR), embeddings,
            allow_dangerous_deserialization=True,
            distance_strategy=DistanceStrategy.COSINE,
        )
        test_q = "ì „ë¶ í˜„ëŒ€ì˜ ìµœê·¼ ì„±ì "
        results = store.similarity_search(test_q, k=3)
        log.info(f"[ê²€ìƒ‰í…ŒìŠ¤íŠ¸] '{test_q}' top-3")
        for i, r in enumerate(results, 1):
            m = r.metadata or {}
            log.info(f" {i}. {m.get('source')} p.{m.get('page_from')} | {r.page_content[:80]}...")
    except Exception as e:
        log.warning(f"ê²€ìƒ‰í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    log.info(f"ğŸ‰ ì™„ë£Œ. ì €ì¥ ìœ„ì¹˜: {VECTORDIR} (index.faiss, index.pkl)")

if __name__ == "__main__":
    main()
