import os
import json
import logging
from typing import List, Dict, Any, Optional
import numpy as np

logger = logging.getLogger(__name__)

class RAGService:
    """RAG ì„œë¹„ìŠ¤ - FAISS ì¸ë±ìŠ¤ë¥¼ í†µí•œ ì •ë³´ ê²€ìƒ‰"""
    
    def __init__(self):
        self.index_path = os.getenv('FAISS_VOLUME_PATH', '/app/vectordb')  # Docker ë³¼ë¥¨ ê²½ë¡œ
        self.index_name = os.getenv('FAISS_INDEX_NAME', 'sr_corpus')
        self.store_name = os.getenv('FAISS_STORE_NAME', 'sr_corpus')
        self.standards_index_name = os.getenv('FAISS_STANDARDS_INDEX_NAME', 'standards')
        self.standards_store_name = os.getenv('FAISS_STANDARDS_STORE_NAME', 'standards')
        
        logger.info(f"ðŸ”§ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”")
        logger.info(f"  - index_path: {self.index_path}")
        logger.info(f"  - index_name: {self.index_name}")
        logger.info(f"  - store_name: {self.store_name}")
        logger.info(f"  - standards_index_name: {self.standards_index_name}")
        logger.info(f"  - standards_store_name: {self.standards_store_name}")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë”© ìƒíƒœ
        self.is_index_loaded = False
        self.faiss_index = None
        self.doc_store = None
        self.standards_faiss_index = None
        self.standards_doc_store = None
        
        # ì¸ë±ìŠ¤ ë¡œë”© ì‹œë„
        self._load_index()
    
    def _load_index(self):
        """FAISS ì¸ë±ìŠ¤ì™€ ë¬¸ì„œ ì €ìž¥ì†Œ ë¡œë”©"""
        try:
            import faiss
            import pickle
            
            # ë©”ì¸ FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ (sr_corpus)
            index_file = os.path.join(self.index_path, self.index_name, "index.faiss")
            store_file = os.path.join(self.index_path, self.store_name, "index.pkl")
            
            # Standards FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
            standards_index_file = os.path.join(self.index_path, self.standards_index_name, "index.faiss")
            standards_store_file = os.path.join(self.index_path, self.standards_store_name, "index.pkl")
            
            logger.info(f"ðŸ” RAG ì„œë¹„ìŠ¤ ì¸ë±ìŠ¤ ë¡œë”© ì‹œìž‘")
            logger.info(f"  - index_path: {self.index_path}")
            logger.info(f"  - index_file: {index_file}")
            logger.info(f"  - store_file: {store_file}")
            logger.info(f"  - standards_index_file: {standards_index_file}")
            logger.info(f"  - standards_store_file: {standards_store_file}")
            
            # ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
            if os.path.exists(self.index_path):
                logger.info(f"ðŸ“ vectordb ë””ë ‰í† ë¦¬ ë‚´ìš©: {os.listdir(self.index_path)}")
                for subdir in os.listdir(self.index_path):
                    subdir_path = os.path.join(self.index_path, subdir)
                    if os.path.isdir(subdir_path):
                        logger.info(f"  ðŸ“ {subdir} ë””ë ‰í† ë¦¬ ë‚´ìš©: {os.listdir(subdir_path)}")
            
            # ë©”ì¸ FAISS ì¸ë±ìŠ¤ ë¡œë”©
            if os.path.exists(index_file) and os.path.exists(store_file):
                try:
                    self.faiss_index = faiss.read_index(index_file)
                    logger.info(f"âœ… ë©”ì¸ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ: {self.faiss_index.ntotal}ê°œ ë¬¸ì„œ")
                    
                    with open(store_file, 'rb') as f:
                        self.doc_store = pickle.load(f)
                    logger.info(f"âœ… ë©”ì¸ ë¬¸ì„œ ì €ìž¥ì†Œ ë¡œë”© ì™„ë£Œ: {len(self.doc_store)}ê°œ ë¬¸ì„œ")
                    
                except Exception as e:
                    logger.error(f"âŒ ë©”ì¸ ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
                    self.faiss_index = None
                    self.doc_store = None
            else:
                logger.warning(f"âš ï¸ ë©”ì¸ FAISS íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŒ: {index_file} ë˜ëŠ” {store_file}")
            
            # Standards FAISS ì¸ë±ìŠ¤ ë¡œë”©
            if os.path.exists(standards_index_file) and os.path.exists(standards_store_file):
                try:
                    self.standards_faiss_index = faiss.read_index(standards_index_file)
                    logger.info(f"âœ… Standards FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ: {self.standards_faiss_index.ntotal}ê°œ ë¬¸ì„œ")
                    
                    with open(standards_store_file, 'rb') as f:
                        self.standards_doc_store = pickle.load(f)
                    logger.info(f"âœ… Standards ë¬¸ì„œ ì €ìž¥ì†Œ ë¡œë”© ì™„ë£Œ: {len(self.standards_doc_store)}ê°œ ë¬¸ì„œ")
                    
                except Exception as e:
                    logger.error(f"âŒ Standards ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
                    self.standards_faiss_index = None
                    self.standards_doc_store = None
            else:
                logger.warning(f"âš ï¸ Standards FAISS íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŒ: {standards_index_file} ë˜ëŠ” {standards_store_file}")
            
            # ìµœì†Œí•œ í•˜ë‚˜ì˜ ì¸ë±ìŠ¤ë¼ë„ ë¡œë“œë˜ì—ˆìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            if self.faiss_index or self.standards_faiss_index:
                self.is_index_loaded = True
                logger.info("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ")
            else:
                self.is_index_loaded = False
                logger.error("âŒ ëª¨ë“  FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨")
            
        except Exception as e:
            logger.error(f"FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            self.is_index_loaded = False
    
    def _extract_text_from_doc(self, doc_content) -> str:
        """ë¬¸ì„œ ê°ì²´ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ (ê°•í™”ëœ ë²„ì „)"""
        try:
            # LangChain Document ê°ì²´ì¸ ê²½ìš°
            if hasattr(doc_content, 'page_content'):
                return doc_content.page_content
            
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
            elif isinstance(doc_content, dict):
                # page_content í‚¤ê°€ ìžˆëŠ” ê²½ìš°
                if 'page_content' in doc_content:
                    return doc_content['page_content']
                # text í‚¤ê°€ ìžˆëŠ” ê²½ìš°
                elif 'text' in doc_content:
                    return doc_content['text']
                # content í‚¤ê°€ ìžˆëŠ” ê²½ìš°
                elif 'content' in doc_content:
                    return doc_content['content']
                # ë‹¤ë¥¸ í‚¤ë“¤ì„ ë¬¸ìžì—´ë¡œ ë³€í™˜
                else:
                    return str(doc_content)
            
            # ë¬¸ìžì—´ì¸ ê²½ìš°
            elif isinstance(doc_content, str):
                return doc_content
            
            # InMemoryDocstore ê°ì²´ì¸ ê²½ìš° - ë” ê°•ë ¥í•œ ì¶”ì¶œ
            elif hasattr(doc_content, '_dict'):
                try:
                    doc_dict = doc_content._dict
                    logger.info(f"ðŸ” InMemoryDocstore ë‚´ë¶€ êµ¬ì¡° ë¶„ì„: {type(doc_dict)}, ê¸¸ì´: {len(doc_dict) if doc_dict else 0}")
                    
                    if doc_dict:
                        # ëª¨ë“  ë¬¸ì„œì˜ ë‚´ìš©ì„ ìˆ˜ì§‘
                        all_texts = []
                        for doc_id, doc_obj in doc_dict.items():
                            logger.info(f"  ðŸ“„ ë¬¸ì„œ ID: {doc_id}, íƒ€ìž…: {type(doc_obj)}")
                            
                            if hasattr(doc_obj, 'page_content'):
                                text_content = doc_obj.page_content
                                logger.info(f"    âœ… page_content ë°œê²¬: {len(text_content)}ìž")
                                all_texts.append(text_content)
                            elif isinstance(doc_obj, dict):
                                if 'page_content' in doc_obj:
                                    text_content = doc_obj['page_content']
                                    logger.info(f"    âœ… dict.page_content ë°œê²¬: {len(text_content)}ìž")
                                    all_texts.append(text_content)
                                elif 'text' in doc_obj:
                                    text_content = doc_obj['text']
                                    logger.info(f"    âœ… dict.text ë°œê²¬: {len(text_content)}ìž")
                                    all_texts.append(text_content)
                                else:
                                    logger.info(f"    âš ï¸ dictì—ì„œ í…ìŠ¤íŠ¸ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {list(doc_obj.keys())}")
                                    all_texts.append(str(doc_obj))
                            elif isinstance(doc_obj, str):
                                logger.info(f"    âœ… ë¬¸ìžì—´ ë°œê²¬: {len(doc_obj)}ìž")
                                all_texts.append(doc_obj)
                            else:
                                logger.info(f"    âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ìž…: {type(doc_obj)}")
                                all_texts.append(str(doc_obj))
                        
                        if all_texts:
                            # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ë‚´ìš© ë°˜í™˜ (ì „ì²´ê°€ ë„ˆë¬´ ê¸¸ ìˆ˜ ìžˆìŒ)
                            first_text = all_texts[0][:1000]  # 1000ìžë¡œ ì œí•œ
                            logger.info(f"âœ… InMemoryDocstoreì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(first_text)}ìž")
                            return first_text
                        else:
                            logger.warning("âš ï¸ InMemoryDocstoreì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ")
                            return str(doc_content)
                    else:
                        logger.warning("âš ï¸ InMemoryDocstore._dictê°€ ë¹„ì–´ìžˆìŒ")
                        return str(doc_content)
                except Exception as e:
                    logger.warning(f"InMemoryDocstore ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    return str(doc_content)
            
            # UUID ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° - ë” ê°•ë ¥í•œ ì¶”ì¶œ
            elif hasattr(doc_content, 'get') and hasattr(doc_content, 'values'):
                try:
                    logger.info(f"ðŸ” UUID ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë¶„ì„: {type(doc_content)}, ê¸¸ì´: {len(doc_content)}")
                    
                    # ëª¨ë“  ê°’ì˜ ë‚´ìš©ì„ ìˆ˜ì§‘
                    all_texts = []
                    for doc_id, doc_obj in doc_content.items():
                        logger.info(f"  ðŸ“„ UUID: {doc_id}, íƒ€ìž…: {type(doc_obj)}")
                        
                        if hasattr(doc_obj, 'page_content'):
                            text_content = doc_obj.page_content
                            logger.info(f"    âœ… page_content ë°œê²¬: {len(text_content)}ìž")
                            all_texts.append(text_content)
                        elif isinstance(doc_obj, dict):
                            if 'page_content' in doc_obj:
                                text_content = doc_obj['page_content']
                                logger.info(f"    âœ… dict.page_content ë°œê²¬: {len(text_content)}ìž")
                                all_texts.append(text_content)
                            elif 'text' in doc_obj:
                                text_content = doc_obj['text']
                                logger.info(f"    âœ… dict.text ë°œê²¬: {len(text_content)}ìž")
                                all_texts.append(text_content)
                            else:
                                logger.info(f"    âš ï¸ dictì—ì„œ í…ìŠ¤íŠ¸ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {list(doc_obj.keys())}")
                                all_texts.append(str(doc_obj))
                        elif isinstance(doc_obj, str):
                            logger.info(f"    âœ… ë¬¸ìžì—´ ë°œê²¬: {len(doc_obj)}ìž")
                            all_texts.append(doc_obj)
                        else:
                            logger.info(f"    âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ìž…: {type(doc_obj)}")
                            all_texts.append(str(doc_obj))
                    
                    if all_texts:
                        # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ë‚´ìš© ë°˜í™˜
                        first_text = all_texts[0][:1000]  # 1000ìžë¡œ ì œí•œ
                        logger.info(f"âœ… UUID ë§¤í•‘ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(first_text)}ìž")
                        return first_text
                    else:
                        logger.warning("âš ï¸ UUID ë§¤í•‘ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ")
                        return str(doc_content)
                except Exception as e:
                    logger.warning(f"UUID ë§¤í•‘ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    return str(doc_content)
            
            # ê¸°íƒ€ ê°ì²´ì¸ ê²½ìš°
            else:
                return str(doc_content)
                
        except Exception as e:
            logger.warning(f"ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return str(doc_content)
    
    def _search_in_doc_store(self, doc_store, query_tokens: List[str], store_type: str) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì €ìž¥ì†Œì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰"""
        relevant_docs = []
        
        try:
            # ë¬¸ì„œ ì €ìž¥ì†Œ íƒ€ìž… í™•ì¸ ë° ì•ˆì „í•œ ì²˜ë¦¬
            if isinstance(doc_store, dict):
                # dict í˜•íƒœì¸ ê²½ìš°
                for doc_id, doc_content in doc_store.items():
                    # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
                    actual_text = self._extract_text_from_doc(doc_content)
                    
                    # ê°œì„ ëœ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                    score = self._calculate_relevance_score(query_tokens, actual_text)
                    
                    # ë””ë²„ê¹…: ëª¨ë“  ë¬¸ì„œì˜ ì ìˆ˜ ì¶œë ¥
                    logger.info(f"ðŸ“„ {store_type} ë¬¸ì„œ {doc_id}: ì ìˆ˜={score}, ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°={actual_text[:100]}...")
                    
                    if score > 0:  # ìž„ê³„ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ë¬¸ì„œ í¬í•¨
                        relevant_docs.append({
                            'content': actual_text,
                            'score': score,
                            'source': f'{store_type}_Document_{doc_id}',
                            'metadata': {
                                'category': 'TCFD',
                                'type': store_type
                            }
                        })
            elif isinstance(doc_store, (list, tuple)):
                # listë‚˜ tuple í˜•íƒœì¸ ê²½ìš°
                logger.info(f"{store_type} ë¬¸ì„œ ì €ìž¥ì†Œê°€ {type(doc_store).__name__} í˜•íƒœë¡œ ë¡œë”©ë¨")
                for i, doc_content in enumerate(doc_store):
                    # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
                    actual_text = self._extract_text_from_doc(doc_content)
                    
                    # ê°œì„ ëœ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                    score = self._calculate_relevance_score(query_tokens, actual_text)
                    
                    # ë””ë²„ê¹…: ëª¨ë“  ë¬¸ì„œì˜ ì ìˆ˜ ì¶œë ¥
                    logger.info(f"ðŸ“„ {store_type} ë¬¸ì„œ {i}: ì ìˆ˜={score}, ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°={actual_text[:100]}...")
                    
                    if score > 0:  # ìž„ê³„ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ë¬¸ì„œ í¬í•¨
                        relevant_docs.append({
                            'content': actual_text,
                            'score': score,
                            'source': f'{store_type}_Document_{i}',
                            'metadata': {
                                'category': 'TCFD',
                                'type': store_type
                            }
                        })
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” {store_type} ë¬¸ì„œ ì €ìž¥ì†Œ íƒ€ìž…: {type(doc_store)}")
                return []
                
        except Exception as e:
            logger.error(f"{store_type} ë¬¸ì„œ ì €ìž¥ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
        
        return relevant_docs
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            if not self.is_index_loaded:
                logger.warning("FAISS ì¸ë±ìŠ¤ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë”ë¯¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return self._get_dummy_results(query, top_k)
            
            # ë¬¸ì„œ ì €ìž¥ì†Œ í™•ì¸
            if self.doc_store is None and self.standards_doc_store is None:
                logger.warning("âš ï¸ ëª¨ë“  ë¬¸ì„œ ì €ìž¥ì†Œ(PKL)ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë”ë¯¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return self._get_dummy_results(query, top_k)
            
            # ì‹¤ì œ FAISS ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
            logger.info(f"ì¿¼ë¦¬ ê²€ìƒ‰: '{query}' (top_k: {top_k})")
            
            # ë©”ì¸ ë¬¸ì„œ ì €ìž¥ì†Œ ìƒíƒœ
            if self.doc_store:
                logger.info(f"ðŸ“š ë©”ì¸ ë¬¸ì„œ ì €ìž¥ì†Œ ìƒíƒœ: {len(self.doc_store)}ê°œ ë¬¸ì„œ")
            else:
                logger.info("âš ï¸ ë©”ì¸ ë¬¸ì„œ ì €ìž¥ì†Œê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            # Standards ë¬¸ì„œ ì €ìž¥ì†Œ ìƒíƒœ
            if self.standards_doc_store:
                logger.info(f"ðŸ“š Standards ë¬¸ì„œ ì €ìž¥ì†Œ ìƒíƒœ: {len(self.standards_doc_store)}ê°œ ë¬¸ì„œ")
            else:
                logger.info("âš ï¸ Standards ë¬¸ì„œ ì €ìž¥ì†Œê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            # ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ TF-IDF ìŠ¤íƒ€ì¼)
            query_tokens = query.lower().split()
            
            # ë¬¸ì„œ ì €ìž¥ì†Œì—ì„œ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ê²€ìƒ‰
            relevant_docs = []
            
            # ë©”ì¸ ë¬¸ì„œ ì €ìž¥ì†Œ ê²€ìƒ‰
            if self.doc_store:
                logger.info("ðŸ” ë©”ì¸ ë¬¸ì„œ ì €ìž¥ì†Œ ê²€ìƒ‰ ì‹œìž‘")
                if isinstance(self.doc_store, (list, tuple)):
                    logger.info(f"ðŸ“š ë©”ì¸ ë¬¸ì„œ ì €ìž¥ì†Œê°€ {type(self.doc_store).__name__} í˜•íƒœë¡œ ë¡œë”©ë¨")
                    for i, doc_content in enumerate(self.doc_store):
                        # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
                        actual_text = self._extract_text_from_doc(doc_content)
                        
                        # ê°œì„ ëœ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                        score = self._calculate_relevance_score(query_tokens, actual_text)
                        
                        # ë””ë²„ê¹…: ëª¨ë“  ë¬¸ì„œì˜ ì ìˆ˜ ì¶œë ¥
                        logger.info(f"ðŸ“„ ë©”ì¸ ë¬¸ì„œ {i}: ì ìˆ˜={score}, ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°={actual_text[:100]}...")
                        
                        if score > 0:  # ìž„ê³„ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ë¬¸ì„œ í¬í•¨
                            relevant_docs.append({
                                'content': actual_text,
                                'score': score,
                                'source': f'main_Document_{i}',
                                'metadata': {
                                    'category': 'TCFD',
                                    'type': 'main'
                                }
                            })
                else:
                    relevant_docs.extend(self._search_in_doc_store(self.doc_store, query_tokens, "main"))
            
            # Standards ë¬¸ì„œ ì €ìž¥ì†Œ ê²€ìƒ‰
            if self.standards_doc_store:
                logger.info("ðŸ” Standards ë¬¸ì„œ ì €ìž¥ì†Œ ê²€ìƒ‰ ì‹œìž‘")
                if isinstance(self.standards_doc_store, (list, tuple)):
                    logger.info(f"ðŸ“š Standards ë¬¸ì„œ ì €ìž¥ì†Œê°€ {type(self.standards_doc_store).__name__} í˜•íƒœë¡œ ë¡œë”©ë¨")
                    for i, doc_content in enumerate(self.standards_doc_store):
                        # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
                        actual_text = self._extract_text_from_doc(doc_content)
                        
                        # ê°œì„ ëœ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                        score = self._calculate_relevance_score(query_tokens, actual_text)
                        
                        # ë””ë²„ê¹…: ëª¨ë“  ë¬¸ì„œì˜ ì ìˆ˜ ì¶œë ¥
                        logger.info(f"ðŸ“„ Standards ë¬¸ì„œ {i}: ì ìˆ˜={score}, ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°={actual_text[:100]}...")
                        
                        if score > 0:  # ìž„ê³„ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ë¬¸ì„œ í¬í•¨
                            relevant_docs.append({
                                'content': actual_text,
                                'score': score,
                                'source': f'standards_Document_{i}',
                                'metadata': {
                                    'category': 'TCFD',
                                    'type': 'standards'
                                }
                            })
                else:
                    relevant_docs.extend(self._search_in_doc_store(self.standards_doc_store, query_tokens, "standards"))
            
            if not relevant_docs:
                logger.warning("âš ï¸ ëª¨ë“  ë¬¸ì„œ ì €ìž¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return self._get_dummy_results(query, top_k)
            
            # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  top_kë§Œ ë°˜í™˜
            relevant_docs.sort(key=lambda x: x['score'], reverse=True)
            results = relevant_docs[:top_k]
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸
            logger.info(f"ðŸ” ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:")
            logger.info(f"  - ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(self.doc_store)}")
            logger.info(f"  - ê´€ë ¨ ë¬¸ì„œ í›„ë³´: {len(relevant_docs)}")
            logger.info(f"  - ìµœì¢… ê²°ê³¼: {len(results)}")
            
            if results:
                for i, doc in enumerate(results):
                    logger.info(f"  ðŸ“„ ê²°ê³¼ {i+1}: ì ìˆ˜={doc['score']}, ì†ŒìŠ¤={doc['source']}")
            
            if not results:
                logger.info("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë”ë¯¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return self._get_dummy_results(query, top_k)
            
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬")
            return results
            
        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self._get_dummy_results(query, top_k)
    
    def _calculate_relevance_score(self, query_tokens: List[str], doc_content: str) -> float:
        """ê°œì„ ëœ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            doc_content_lower = doc_content.lower()
            doc_tokens = doc_content_lower.split()
            
            # ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ë¶€ë¶„ ë§¤ì¹­ë„ ê³ ë ¤)
            basic_score = 0
            for token in query_tokens:
                if token in doc_content_lower:
                    basic_score += 1
                else:
                    # ë¶€ë¶„ ë§¤ì¹­ ì ìˆ˜ (í† í°ì˜ ì¼ë¶€ê°€ í¬í•¨ëœ ê²½ìš°)
                    for doc_token in doc_tokens:
                        if len(token) > 2 and (token in doc_token or doc_token in token):
                            basic_score += 0.3
                            break
            
            # ê°€ì¤‘ì¹˜ ê³„ì‚°
            # 1. TCFD ê´€ë ¨ í‚¤ì›Œë“œì— ë†’ì€ ê°€ì¤‘ì¹˜
            tcfd_keywords = ['tcfd', 'ê¸°í›„', 'ê¸°í›„ë³€í™”', 'íƒ„ì†Œ', 'ì˜¨ì‹¤ê°€ìŠ¤', 'esg', 'ì§€ì†ê°€ëŠ¥', 'ìž¬ë¬´', 'ê³µì‹œ', 'ìœ„í—˜', 'ê¸°íšŒ']
            tcfd_weight = 0
            for keyword in tcfd_keywords:
                if keyword in doc_content_lower:
                    tcfd_weight += 0.3
            
            # 2. íšŒì‚¬ëª… ë§¤ì¹­ì— ë†’ì€ ê°€ì¤‘ì¹˜
            company_keywords = ['í•œì˜¨ì‹œìŠ¤í…œ', 'í˜„ëŒ€ëª¨ë¹„ìŠ¤', 'hlë§Œë„', 'ê¸ˆí˜¸íƒ€ì´ì–´']
            company_weight = 0
            for company in company_keywords:
                if company in doc_content_lower:
                    company_weight += 1.0
            
            # 3. ë¬¸ì„œ ê¸¸ì´ì— ë”°ë¥¸ ì •ê·œí™” (ë” ê´€ëŒ€í•˜ê²Œ)
            length_factor = min(1.0, len(doc_content) / 500)  # 500ìž ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê¸°ë³¸ ì ìˆ˜ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            final_score = (basic_score * 0.5 + tcfd_weight * 0.3 + company_weight * 0.2) * length_factor
            
            return round(final_score, 3)
            
        except Exception as e:
            logger.warning(f"ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _get_dummy_results(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ë”ë¯¸ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜ (í…ŒìŠ¤íŠ¸ìš©)"""
        dummy_results = []
        
        # TCFD ê´€ë ¨ ë”ë¯¸ ë°ì´í„°
        tcfd_content = [
            "TCFD(Task Force on Climate-related Financial Disclosures)ëŠ” ê¸°í›„ ê´€ë ¨ ìž¬ë¬´ì •ë³´ ê³µì‹œë¥¼ ìœ„í•œ êµ­ì œ í‘œì¤€ í”„ë ˆìž„ì›Œí¬ìž…ë‹ˆë‹¤.",
            "ê±°ë²„ë„ŒìŠ¤ ì˜ì—­ì—ì„œëŠ” ê¸°í›„ ê´€ë ¨ ìœ„í—˜ê³¼ ê¸°íšŒì— ëŒ€í•œ ì´ì‚¬íšŒ ê°ë… ë° ê²½ì˜ì§„ ì—­í• ì„ ëª…í™•ížˆ í•´ì•¼ í•©ë‹ˆë‹¤.",
            "ì „ëžµ ì˜ì—­ì—ì„œëŠ” ê¸°í›„ ê´€ë ¨ ìœ„í—˜ê³¼ ê¸°íšŒê°€ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ê³  ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.",
            "ìœ„í—˜ê´€ë¦¬ ì˜ì—­ì—ì„œëŠ” ê¸°í›„ ê´€ë ¨ ìœ„í—˜ì„ ì‹ë³„, í‰ê°€, ê´€ë¦¬í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ì „ì‚¬ì  ìœ„í—˜ê´€ë¦¬ ì²´ê³„ì— í†µí•©í•´ì•¼ í•©ë‹ˆë‹¤.",
            "ì§€í‘œ ë° ëª©í‘œ ì˜ì—­ì—ì„œëŠ” ê¸°í›„ ê´€ë ¨ ìœ„í—˜ê³¼ ê¸°íšŒë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œë¥¼ ì„¤ì •í•˜ê³  êµ¬ì²´ì ì¸ ëª©í‘œë¥¼ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤."
        ]
        
        for i in range(min(top_k, len(tcfd_content))):
            dummy_results.append({
                'content': tcfd_content[i],
                'score': 0.9 - (i * 0.1),
                'source': f'TCFD_Standard_{i+1}',
                'metadata': {
                    'category': 'TCFD',
                    'type': 'standard'
                }
            })
        
        return dummy_results
    
    def get_service_status(self) -> Dict[str, Any]:
        """RAG ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
        return {
            'is_loaded': self.is_index_loaded,
            'index_path': self.index_path,
            'index_name': self.index_name,
            'store_name': self.store_name
        }

    def search_openai(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """OpenAIìš© RAG ê²€ìƒ‰ (TCFD ë³´ê³ ì„œ ì„œë¹„ìŠ¤ í˜¸í™˜ì„±)"""
        return self.search(query, top_k)
    
    def search_huggingface(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Hugging Faceìš© RAG ê²€ìƒ‰ (TCFD ë³´ê³ ì„œ ì„œë¹„ìŠ¤ í˜¸í™˜ì„±)"""
        return self.search(query, top_k)
