import faiss
import numpy as np
import pickle
import logging
import requests
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
from ...common.config import (
    get_faiss_index_path, get_faiss_store_path,
    EMBED_DIM, HF_API_TOKEN, HF_MODEL, HF_API_URL
)
from ...common.schemas import SearchHit
from .base_rag_service import BaseRAGService
from ..llm.huggingface_llm_service import HuggingFaceLLMService

logger = logging.getLogger(__name__)

class HuggingFaceRAGService(BaseRAGService):
    """Hugging Face ê¸°ë°˜ RAG ì„œë¹„ìŠ¤ (ì½”ì•ŒíŒŒ, RoLA í•™ìŠµìš©)"""
    
    def __init__(self):
        super().__init__("Hugging Face RAG Service")
        self.index: Optional[faiss.Index] = None
        self.doc_store: Optional[List[Dict[str, Any]]] = None
        self.llm_service = HuggingFaceLLMService()
        
        # Hugging Face API ì„¤ì • ê²€ì¦
        if not HF_API_TOKEN:
            logger.warning("Hugging Face API í† í°ì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        if not HF_API_URL:
            logger.warning("Hugging Face API URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
    
    def load_index(self) -> bool:
        """FAISS ì¸ë±ìŠ¤ì™€ ë¬¸ì„œ ì €ì¥ì†Œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            if not get_faiss_index_path().exists():
                logger.warning(f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {get_faiss_index_path()}")
                return False
            
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            self.index = faiss.read_index(str(get_faiss_index_path()))
            logger.info(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")
            
            # ë¬¸ì„œ ì €ì¥ì†Œ ë¡œë”© ì‹œë„
            try:
                store_path = get_faiss_store_path()
                logger.info(f"ğŸ“– PKL ë¬¸ì„œ ì €ì¥ì†Œ ë¡œë”© ì‹œë„: {store_path}")
                
                if store_path.exists():
                    with open(store_path, 'rb') as f:
                        self.doc_store = pickle.load(f)
                    logger.info(f"âœ… ë¬¸ì„œ ì €ì¥ì†Œ ë¡œë”© ì™„ë£Œ: {len(self.doc_store)}ê°œ ë¬¸ì„œ")
                else:
                    logger.warning(f"âš ï¸ ë¬¸ì„œ ì €ì¥ì†Œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {store_path}")
                    self.doc_store = None
            except Exception as pkl_error:
                logger.error(f"âŒ PKL íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {str(pkl_error)}")
                
                # Pydantic í˜¸í™˜ì„± ë¬¸ì œ ì‹œë„ í•´ê²°
                if '__fields_set__' in str(pkl_error):
                    logger.info("ğŸ”„ Pydantic v1/v2 í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€, ëŒ€ì²´ ë°©ë²• ì‹œë„")
                    try:
                        # pickle5 ë˜ëŠ” ë‹¤ë¥¸ ë¡œë” ì‹œë„
                        import pickle5
                        with open(store_path, 'rb') as f:
                            self.doc_store = pickle5.load(f)
                        logger.info(f"âœ… pickle5ë¡œ ë¬¸ì„œ ì €ì¥ì†Œ ë¡œë”© ì„±ê³µ: {len(self.doc_store)}ê°œ ë¬¸ì„œ")
                    except ImportError:
                        logger.info("pickle5ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ, Pydantic v2 í˜¸í™˜ì„± ì²˜ë¦¬ ì‹œë„")
                        try:
                            # Pydantic v1 ê°ì²´ë¥¼ v2ë¡œ ë³€í™˜í•˜ëŠ” ì‹œë„
                            with open(store_path, 'rb') as f:
                                raw_data = pickle.load(f)
                            
                            # v1 ê°ì²´ì˜ __fields_set__ ë¬¸ì œ í•´ê²°
                            if isinstance(raw_data, dict):
                                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜ ì‹œë„
                                converted_data = {}
                                for key, value in raw_data.items():
                                    if hasattr(value, '__dict__'):
                                        # ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                                        converted_data[key] = value.__dict__
                                    else:
                                        converted_data[key] = value
                                self.doc_store = converted_data
                                logger.info(f"âœ… Pydantic v1/v2 í˜¸í™˜ì„± ì²˜ë¦¬ë¡œ ë¬¸ì„œ ì €ì¥ì†Œ ë¡œë”© ì„±ê³µ: {len(self.doc_store)}ê°œ ë¬¸ì„œ")
                            else:
                                raise Exception("ë°ì´í„° í˜•íƒœ ë³€í™˜ ì‹¤íŒ¨")
                                
                        except Exception as compat_error:
                            logger.error(f"âŒ Pydantic í˜¸í™˜ì„± í•´ê²° ì‹œë„ ì‹¤íŒ¨: {str(compat_error)}")
                            self.doc_store = None
                            logger.warning("âš ï¸ ë¬¸ì„œ ì €ì¥ì†Œ ì—†ì´ FAISS ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©")
                    except Exception as pkl5_error:
                        logger.error(f"âŒ pickle5 ë¡œë”©ë„ ì‹¤íŒ¨: {str(pkl5_error)}")
                        self.doc_store = None
                        logger.warning("âš ï¸ ë¬¸ì„œ ì €ì¥ì†Œ ì—†ì´ FAISS ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©")
                else:
                    self.doc_store = None
                    logger.warning("âš ï¸ ë¬¸ì„œ ì €ì¥ì†Œ ì—†ì´ FAISS ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©")
            
            # ì°¨ì› ê²€ì¦
            if self.index.d != EMBED_DIM:
                logger.error(f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: ì¸ë±ìŠ¤={self.index.d}, ì„¤ì •={EMBED_DIM}")
                return False
            
            self.is_loaded = True
            if self.doc_store:
                logger.info("Hugging Face RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (FAISS ì¸ë±ìŠ¤ + ë¬¸ì„œ ì €ì¥ì†Œ)")
            else:
                logger.info("Hugging Face RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (FAISS ì¸ë±ìŠ¤ë§Œ)")
            return True
            
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def search(self, query: str, top_k: int = 5) -> Tuple[List[SearchHit], str]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.is_loaded:
            raise RuntimeError("RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        
        try:
            hits = []
            context_parts = []
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ìƒ‰
            query_lower = query.lower()
            
            for idx, doc in enumerate(self.doc_store):
                text = doc.get('text', '').lower()
                if query_lower in text:
                    # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°
                    score = 1.0 / (1.0 + len(text.split()))
                    
                    hit = SearchHit(
                        rank=len(hits) + 1,
                        id=str(idx),
                        score=score,
                        text=doc.get('text', ''),
                        meta=doc.get('meta', {})
                    )
                    hits.append(hit)
                    
                    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                    context_part = f"[{len(hits)}] {doc.get('text', '')}"
                    if doc.get('meta', {}).get('source'):
                        context_part += f" (ì¶œì²˜: {doc['meta']['source']})"
                    context_parts.append(context_part)
                    
                    if len(hits) >= top_k:
                        break
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            hits.sort(key=lambda x: x.score, reverse=True)
            
            # ì»¨í…ìŠ¤íŠ¸ ì—°ê²°
            context = "\n\n---\n\n".join(context_parts)
            
            logger.info(f"Hugging Face RAG ê²€ìƒ‰ ì™„ë£Œ: {len(hits)}ê°œ ê²°ê³¼, top_k={top_k}")
            return hits, context
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise
    
    def generate_draft(self, question: str, sections: List[str], top_k: int = 8) -> str:
        """ì„¹ì…˜ë³„ ì´ˆì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.is_loaded:
            raise RuntimeError("RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            hits, context = self.search(question, top_k)
            
            # ê° ì„¹ì…˜ë³„ ì´ˆì•ˆ ìƒì„±
            draft_parts = []
            for section in sections:
                try:
                    draft_content = self.llm_service.generate_draft_section(
                        question=question,
                        context=context,
                        section=section
                    )
                    draft_parts.append(f"## {section}\n\n{draft_content}")
                except Exception as e:
                    logger.error(f"ì„¹ì…˜ {section} ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
                    draft_parts.append(f"## {section}\n\nì´ˆì•ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            return "\n\n".join(draft_parts)
            
        except Exception as e:
            logger.error(f"ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def polish_text(self, text: str, tone: str = "ê³µì‹ì ", style_guide: str = "") -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ìœ¤ë¬¸í•©ë‹ˆë‹¤."""
        try:
            return self.llm_service.polish_text(text, tone, style_guide)
        except Exception as e:
            logger.error(f"ìœ¤ë¬¸ ì‹¤íŒ¨: {e}")
            raise
