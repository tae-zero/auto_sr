"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤
SR PDFì™€ TCFD ê¸°ì¤€ì„œ ì„ë² ë”© ë° ê²€ìƒ‰ ê¸°ëŠ¥
"""

import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from shutil import rmtree
import chromadb
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.schema import Document
import pandas as pd
import os

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self, base_path: str = None, chroma_path: str = None, 
                 device: str = None, force_recreate: bool = False):
        
        # í™˜ê²½ì— ë”°ë¥¸ ê²½ë¡œ ì„¤ì •
        if base_path is None:
            if os.getenv("RAILWAY_ENVIRONMENT") == "true":
                base_path = "/app/document"  # Railway í™˜ê²½
            else:
                base_path = "../../document"  # ë¡œì»¬ ê°œë°œ í™˜ê²½
                
        if chroma_path is None:
            if os.getenv("RAILWAY_ENVIRONMENT") == "true":
                chroma_path = "/app/chroma_db"  # Railway í™˜ê²½
            else:
                chroma_path = "./chroma_db"  # ë¡œì»¬ ê°œë°œ í™˜ê²½
        self.base_path = Path(base_path)
        self.chroma_path = Path(chroma_path)
        self.chroma_path.mkdir(exist_ok=True)
        self.force_recreate = force_recreate
        
        # GPU ì‚¬ìš© ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ)
        if device is None:
            device = os.getenv("EMBEDDING_DEVICE", "cpu")
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (E5 ëª¨ë¸ ìµœì í™”)
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-base",
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (ê¶Œì¥ íŒŒë¼ë¯¸í„°)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            length_function=len,
            separators=["\n## ", "\n#", "\n\n", "\n", " "]
        )
        
        # ë²¡í„°ìŠ¤í† ì–´ ìºì‹œ
        self._vectorstores = {}
        
        logger.info(f"RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (device: {device})")
    
    def _reset_dir(self, path: Path):
        """ë””ë ‰í† ë¦¬ ì´ˆê¸°í™” (force_recreateìš©)"""
        if path.exists():
            for p in path.iterdir():
                if p.is_dir(): 
                    rmtree(p)
                else: 
                    p.unlink()
        path.mkdir(parents=True, exist_ok=True)
    
    async def initialize_embeddings(self) -> bool:
        """ì„ë² ë”© ì´ˆê¸°í™” (ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ í˜¸ì¶œ)"""
        try:
            logger.info("ğŸ” ì„ë² ë”© ì´ˆê¸°í™” ì‹œì‘")
            
            # Railway í™˜ê²½ì—ì„œëŠ” ê¸°ì¡´ ë²¡í„° ìš°ì„  ì‚¬ìš©
            if os.getenv("RAILWAY_ENVIRONMENT") == "true":
                logger.info("ğŸš‚ Railway í™˜ê²½ ê°ì§€ - ê¸°ì¡´ ë²¡í„° ìš°ì„  ì‚¬ìš©")
            
            # SR ì½”í¼ìŠ¤ ìƒì„±
            await self._create_sr_corpus()
            
            # TCFD Standards ìƒì„±
            await self._create_tcfd_standards()
            
            logger.info("âœ… ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_sr_corpus(self):
        """SR PDF ë¬¸ì„œë“¤ì„ ì„ë² ë”©í•˜ì—¬ sr_corpus ìƒì„±"""
        collection_name = "sr_corpus"
        collection_path = self.chroma_path / collection_name
        
        # ì¤‘ë³µ ì¸ë±ì‹± ë°©ì§€ (ì•ˆì „ì¥ì¹˜)
        if collection_path.exists() and not self.force_recreate:
            try:
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ ì‹œë„
                vectorstore = Chroma(
                    collection_name=collection_name,
                    embedding_function=self.embedding_model,
                    persist_directory=str(collection_path)
                )
                self._vectorstores[collection_name] = vectorstore
                logger.info(f"âœ… ê¸°ì¡´ {collection_name} ë¡œë“œ ì™„ë£Œ (ìŠ¤í‚µ)")
                return
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ {collection_name} ë¡œë“œ ì‹¤íŒ¨, ì¬ìƒì„±: {e}")
        
        # force_recreate ì‹œ ë””ë ‰í† ë¦¬ ì´ˆê¸°í™”
        if self.force_recreate:
            self._reset_dir(collection_path)
        
        logger.info("ğŸ”¹ SR ì½”í¼ìŠ¤ ìƒì„± ì‹œì‘")
        documents = await self._load_pdf_documents("sr")
        
        if documents:
            vectorstore = await self._create_vectorstore(documents, collection_name)
            if vectorstore:
                self._vectorstores[collection_name] = vectorstore
                logger.info("âœ… SR ì½”í¼ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    async def _create_tcfd_standards(self):
        """TCFD ê¸°ì¤€ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ standards ìƒì„±"""
        collection_name = "standards"
        collection_path = self.chroma_path / collection_name
        
        # ì¤‘ë³µ ì¸ë±ì‹± ë°©ì§€ (ì•ˆì „ì¥ì¹˜)
        if collection_path.exists() and not self.force_recreate:
            try:
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ ì‹œë„
                vectorstore = Chroma(
                    collection_name=collection_name,
                    embedding_function=self.embedding_model,
                    persist_directory=str(collection_path)
                )
                self._vectorstores[collection_name] = vectorstore
                logger.info(f"âœ… ê¸°ì¡´ {collection_name} ë¡œë“œ ì™„ë£Œ (ìŠ¤í‚µ)")
                return
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ {collection_name} ë¡œë“œ ì‹¤íŒ¨, ì¬ìƒì„±: {e}")
        
        # force_recreate ì‹œ ë””ë ‰í† ë¦¬ ì´ˆê¸°í™”
        if self.force_recreate:
            self._reset_dir(collection_path)
        
        logger.info("ğŸ”¹ TCFD Standards ìƒì„± ì‹œì‘")
        documents = await self._load_pdf_documents("tcfd")
        
        if documents:
            vectorstore = await self._create_vectorstore(documents, collection_name)
            if vectorstore:
                self._vectorstores[collection_name] = vectorstore
                logger.info("âœ… TCFD Standards ìƒì„± ì™„ë£Œ")
    
    async def _load_pdf_documents(self, pdf_dir: str) -> List[Document]:
        """PDF ë¬¸ì„œë“¤ì„ ë¡œë“œ"""
        pdf_path = self.base_path / pdf_dir
        documents = []
        
        if not pdf_path.exists():
            logger.warning(f"PDF ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")
            return documents
        
        # TCFD ë§¤í•‘ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        tcfd_mapping = await self._load_tcfd_mapping()
        
        for pdf_file in pdf_path.glob("*.pdf"):
            try:
                logger.info(f"PDF ë¡œë”© ì¤‘: {pdf_file.name}")
                loader = PyPDFLoader(str(pdf_file))
                pages = loader.load()
                
                # íŒŒì¼ëª…ì—ì„œ íšŒì‚¬ëª…ê³¼ ì—°ë„ ì¶”ì¶œ
                company, year = self._extract_company_year(pdf_file.name)
                
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ê°œì„ ëœ ë²„ì „)
                for i, page in enumerate(pages):
                    metadata = {
                        "collection": pdf_dir,
                        "source": pdf_file.name,
                        "company": company,
                        "year": year,
                        "page_from": i + 1,
                        "page_to": i + 1,
                        "type": "pdf"
                    }
                    
                    # TCFD ë§¤í•‘ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                    if tcfd_mapping and company in tcfd_mapping:
                        metadata.update(tcfd_mapping[company])
                    
                    page.metadata.update(metadata)
                
                documents.extend(pages)
                logger.info(f"âœ… {pdf_file.name} ë¡œë”© ì™„ë£Œ ({len(pages)} í˜ì´ì§€)")
                
            except Exception as e:
                logger.error(f"âŒ {pdf_file.name} ë¡œë”© ì‹¤íŒ¨: {e}")
        
        return documents
    
    def _extract_company_year(self, filename: str) -> tuple:
        """íŒŒì¼ëª…ì—ì„œ íšŒì‚¬ëª…ê³¼ ì—°ë„ ì¶”ì¶œ"""
        parts = filename.replace('.pdf', '').split('_')
        
        if len(parts) >= 2:
            company = parts[0]
            year = parts[-1]
            return company, year
        
        return filename.replace('.pdf', ''), "unknown"
    
    async def _load_tcfd_mapping(self) -> Dict:
        """TCFD ë§¤í•‘ ì •ë³´ ë¡œë“œ"""
        mapping_path = self.base_path / "tcfd_mapping.xlsx"
        
        if not mapping_path.exists():
            logger.info("TCFD ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            df = pd.read_excel(mapping_path)
            mapping = {}
            
            for _, row in df.iterrows():
                company = row.get('company', '')
                if company:
                    mapping[company] = {
                        "pillar": row.get('pillar', ''),
                        "section": row.get('section', '')
                    }
            
            logger.info(f"TCFD ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(mapping)}ê°œ íšŒì‚¬")
            return mapping
            
        except Exception as e:
            logger.error(f"TCFD ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _create_vectorstore(self, documents: List[Document], collection_name: str) -> Optional[Chroma]:
        """ë¬¸ì„œë“¤ì„ ì„ë² ë”©í•˜ì—¬ ChromaDB ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
        if not documents:
            logger.warning(f"ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: {collection_name}")
            return None
        
        # í…ìŠ¤íŠ¸ ë¶„í•  (E5 ëª¨ë¸ ìµœì í™” + ë©”íƒ€ë°ì´í„° ë³µì‚¬)
        logger.info(f"í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘: {collection_name}")
        texts = []
        metadatas = []
        
        for doc in documents:
            chunks = self.text_splitter.split_text(doc.page_content)
            for chunk in chunks:
                # E5 ëª¨ë¸ ìµœì í™”: ë¬¸ì„œì— "passage: " í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
                optimized_text = f"passage: {chunk}"
                texts.append(optimized_text)
                # ë©”íƒ€ë°ì´í„° ë³µì‚¬ë³¸ ìƒì„± (dict ë ˆí¼ëŸ°ìŠ¤ ì¬ì‚¬ìš© ë°©ì§€)
                metadatas.append(dict(doc.metadata))
        
        logger.info(f"âœ… {collection_name}: {len(texts)}ê°œ ì²­í¬ ìƒì„±")
        
        # ChromaDB ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorstore = Chroma.from_texts(
            texts=texts,
            embedding=self.embedding_model,
            metadatas=metadatas,
            collection_name=collection_name,
            persist_directory=str(self.chroma_path / collection_name)
        )
        
        # ì €ì¥
        vectorstore.persist()
        logger.info(f"âœ… {collection_name} ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ")
        
        return vectorstore
    
    async def search_sr_corpus(self, query: str, k: int = 5, filters: Dict = None) -> List[Document]:
        """SR ì½”í¼ìŠ¤ì—ì„œ ê²€ìƒ‰"""
        if "sr_corpus" not in self._vectorstores:
            logger.error("SR ì½”í¼ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # E5 ëª¨ë¸ ìµœì í™”: ì¿¼ë¦¬ì— "query: " í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
            q = f"query: {query}"
            results = self._vectorstores["sr_corpus"].similarity_search(
                q, 
                k=k,
                filter=filters
            )
            logger.info(f"SR ì½”í¼ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results
        except Exception as e:
            logger.error(f"SR ì½”í¼ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def search_standards(self, query: str, k: int = 5, filters: Dict = None) -> List[Document]:
        """TCFD Standardsì—ì„œ ê²€ìƒ‰"""
        if "standards" not in self._vectorstores:
            logger.error("TCFD Standardsê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # E5 ëª¨ë¸ ìµœì í™”: ì¿¼ë¦¬ì— "query: " í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
            q = f"query: {query}"
            results = self._vectorstores["standards"].similarity_search(
                q, 
                k=k,
                filter=filters
            )
            logger.info(f"TCFD Standards ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results
        except Exception as e:
            logger.error(f"TCFD Standards ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def search_all(self, query: str, k: int = 5, filters: Dict = None) -> Dict[str, List[Document]]:
        """ëª¨ë“  ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰"""
        results = {
            "sr_corpus": await self.search_sr_corpus(query, k, filters),
            "standards": await self.search_standards(query, k, filters)
        }
        return results
    
    async def get_collection_info(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        info = {}
        
        for collection_name, vectorstore in self._vectorstores.items():
            try:
                # ChromaDB í´ë¼ì´ì–¸íŠ¸ë¡œ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ
                chroma_path = str(self.chroma_path / collection_name)
                client = chromadb.PersistentClient(path=chroma_path)
                collection = client.get_collection(name=collection_name)
                count = collection.count()
                
                info[collection_name] = {
                    "document_count": count,
                    "status": "active"
                }
                
            except Exception as e:
                logger.warning(f"{collection_name} ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                info[collection_name] = {
                    "document_count": 0,
                    "status": "error"
                }
        
        return info
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("RAG ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬")
        self._vectorstores.clear()
