#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¹ ë¥¸ CSV íŒŒì¼ ë¡œë”© ìŠ¤í¬ë¦½íŠ¸ - Railway PostgreSQLì— ê¸°í›„ ë°ì´í„° ì €ì¥
ë°°ì¹˜ ì²˜ë¦¬ì™€ ì§„í–‰ ìƒí™© í‘œì‹œë¡œ ìµœì í™”
"""

import pandas as pd
import os
import asyncio
import asyncpg
from pathlib import Path
import logging
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FastClimateDataLoader:
    def __init__(self):
        self.pool = None
        self.batch_size = 1000  # ë°°ì¹˜ í¬ê¸° ì¤„ì„
        
    async def connect_to_database(self):
        """Railway PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°"""
        try:
            database_url = os.getenv('DATABASE_URL')
            if not database_url:
                raise ValueError("DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if database_url.startswith('postgresql+asyncpg://'):
                database_url = database_url.replace('postgresql+asyncpg://', 'postgresql://')
            
            logger.info(f"ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
            self.pool = await asyncpg.create_pool(database_url)
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def create_tables(self):
        """ê¸°í›„ ë°ì´í„° í…Œì´ë¸”ë“¤ ìƒì„±"""
        try:
            conn = await self.pool.acquire()
            
            # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ
            await conn.execute("DROP TABLE IF EXISTS climate_data CASCADE")
            await conn.execute("DROP TABLE IF EXISTS administrative_regions CASCADE")
            await conn.execute("DROP TABLE IF EXISTS climate_variables CASCADE")
            await conn.execute("DROP TABLE IF EXISTS climate_scenarios CASCADE")
            
            # í…Œì´ë¸” ìƒì„±
            await conn.execute("""
                CREATE TABLE climate_scenarios (
                    id SERIAL PRIMARY KEY,
                    scenario_code VARCHAR(20) UNIQUE NOT NULL,
                    scenario_name VARCHAR(100) NOT NULL,
                    description TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            await conn.execute("""
                CREATE TABLE climate_variables (
                    id SERIAL PRIMARY KEY,
                    variable_code VARCHAR(20) UNIQUE NOT NULL,
                    variable_name VARCHAR(100) NOT NULL,
                    unit VARCHAR(50) NOT NULL,
                    description TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            await conn.execute("""
                CREATE TABLE administrative_regions (
                    id SERIAL PRIMARY KEY,
                    region_code VARCHAR(20) UNIQUE NOT NULL,
                    region_name VARCHAR(100) NOT NULL,
                    sub_region_name VARCHAR(100) UNIQUE NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            await conn.execute("""
                CREATE TABLE climate_data (
                    id SERIAL PRIMARY KEY,
                    scenario_id INTEGER REFERENCES climate_scenarios(id),
                    variable_id INTEGER REFERENCES climate_variables(id),
                    region_id INTEGER REFERENCES administrative_regions(id),
                    year INTEGER NOT NULL,
                    value FLOAT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            await conn.execute("CREATE INDEX idx_climate_data_lookup ON climate_data(scenario_id, variable_id, region_id, year)")
            
            logger.info("âœ… í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
        finally:
            await self.pool.release(conn)
    
    async def insert_master_data(self):
        """ë§ˆìŠ¤í„° ë°ì´í„° ì‚½ì…"""
        try:
            conn = await self.pool.acquire()
            
            # ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°
            scenarios = [
                ('SSP126', 'SSP1-2.6 (ì €íƒ„ì†Œ ì‹œë‚˜ë¦¬ì˜¤)', 'IPCC AR6 SSP1-2.6 ì‹œë‚˜ë¦¬ì˜¤'),
                ('SSP585', 'SSP5-8.5 (ê³ íƒ„ì†Œ ì‹œë‚˜ë¦¬ì˜¤)', 'IPCC AR6 SSP5-8.5 ì‹œë‚˜ë¦¬ì˜¤')
            ]
            
            for scenario in scenarios:
                await conn.execute("""
                    INSERT INTO climate_scenarios (scenario_code, scenario_name, description)
                    VALUES ($1, $2, $3)
                """, *scenario)
            
            # ë³€ìˆ˜ ë°ì´í„°
            variables = [
                ('HW33', 'í­ì—¼ì¼ìˆ˜', 'ì¼', 'ìµœê³ ê¸°ì˜¨ 33Â°C ì´ìƒì¸ ì¼ìˆ˜'),
                ('RN', 'ì—°ê°•ìˆ˜ëŸ‰', 'mm', 'ì—°ê°„ ì´ ê°•ìˆ˜ëŸ‰'),
                ('TA', 'ì—°í‰ê· ê¸°ì˜¨', 'Â°C', 'ì—°ê°„ í‰ê·  ê¸°ì˜¨'),
                ('TR25', 'ì—´ëŒ€ì•¼ì¼ìˆ˜', 'ì¼', 'ìµœì €ê¸°ì˜¨ 25Â°C ì´ìƒì¸ ì¼ìˆ˜'),
                ('RAIN80', 'í˜¸ìš°ì¼ìˆ˜', 'ì¼', 'ì¼ê°•ìˆ˜ëŸ‰ 80mm ì´ìƒì¸ ì¼ìˆ˜')
            ]
            
            for variable in variables:
                await conn.execute("""
                    INSERT INTO climate_variables (variable_code, variable_name, unit, description)
                    VALUES ($1, $2, $3, $4)
                """, *variable)
            
            logger.info("âœ… ë§ˆìŠ¤í„° ë°ì´í„° ì‚½ì… ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë§ˆìŠ¤í„° ë°ì´í„° ì‚½ì… ì‹¤íŒ¨: {str(e)}")
            raise
        finally:
            await self.pool.release(conn)
    
    async def load_csv_data(self, csv_file_path):
        """CSV íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ (ë°°ì¹˜ ì²˜ë¦¬)"""
        try:
            start_time = time.time()
            logger.info(f"ğŸ“„ CSV íŒŒì¼ ë¡œë“œ ì‹œì‘: {csv_file_path.name}")
            
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(csv_file_path)
            total_rows = len(df)
            logger.info(f"ğŸ“Š CSV ë°ì´í„°: {total_rows}í–‰, {len(df.columns)}ì»¬ëŸ¼")
            
            # íŒŒì¼ëª…ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ì™€ ë³€ìˆ˜ ì¶”ì¶œ
            filename = csv_file_path.stem
            if 'SSP126' in filename:
                scenario_code = 'SSP126'
            elif 'SSP585' in filename:
                scenario_code = 'SSP585'
            else:
                logger.warning(f"âš ï¸ ì‹œë‚˜ë¦¬ì˜¤ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                return False
            
            if 'HW33' in filename:
                variable_code = 'HW33'
            elif 'RN' in filename:
                variable_code = 'RN'
            elif 'TA' in filename:
                variable_code = 'TA'
            elif 'TR25' in filename:
                variable_code = 'TR25'
            elif 'RAIN80' in filename:
                variable_code = 'RAIN80'
            else:
                logger.warning(f"âš ï¸ ë³€ìˆ˜ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                return False
            
            logger.info(f"ğŸ” ì‹œë‚˜ë¦¬ì˜¤: {scenario_code}, ë³€ìˆ˜: {variable_code}")
            
            conn = await self.pool.acquire()
            
            # ì‹œë‚˜ë¦¬ì˜¤ IDì™€ ë³€ìˆ˜ ID ê°€ì ¸ì˜¤ê¸°
            scenario_id = await conn.fetchval(
                "SELECT id FROM climate_scenarios WHERE scenario_code = $1", 
                scenario_code
            )
            variable_id = await conn.fetchval(
                "SELECT id FROM climate_variables WHERE variable_code = $1", 
                variable_code
            )
            
            if not scenario_id or not variable_id:
                logger.error(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ ë˜ëŠ” ë³€ìˆ˜ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œë§Œ í–‰ì •êµ¬ì—­ ë°ì´í„° ì‚½ì…
            if 'HW33' in filename and 'SSP126' in filename:
                logger.info("ğŸ—ï¸ í–‰ì •êµ¬ì—­ ë°ì´í„° ì‚½ì… ì¤‘...")
                unique_regions = df['Sub_Region_Name'].unique()
                for i, region_name in enumerate(unique_regions):
                    await conn.execute("""
                        INSERT INTO administrative_regions (region_code, region_name, sub_region_name)
                        VALUES ($1, $2, $3)
                    """, f"REG_{i+1:03d}", "ëŒ€í•œë¯¼êµ­", region_name)
                logger.info(f"âœ… í–‰ì •êµ¬ì—­ ë°ì´í„° ì‚½ì… ì™„ë£Œ: {len(unique_regions)}ê°œ")
            
            # ê¸°í›„ ë°ì´í„° ë°°ì¹˜ ì‚½ì…
            logger.info("ğŸ“Š ê¸°í›„ ë°ì´í„° ì‚½ì… ì‹œì‘...")
            
            # í–‰ì •êµ¬ì—­ ID ë§¤í•‘ ìƒì„±
            region_mapping = {}
            for _, row in df.iterrows():
                region_name = row['Sub_Region_Name']
                if region_name not in region_mapping:
                    region_id = await conn.fetchval(
                        "SELECT id FROM administrative_regions WHERE sub_region_name = $1",
                        region_name
                    )
                    region_mapping[region_name] = region_id
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì‚½ì…
            data_to_insert = []
            inserted_count = 0
            
            for _, row in df.iterrows():
                region_id = region_mapping.get(row['Sub_Region_Name'])
                if region_id:
                    data_to_insert.append((
                        scenario_id,
                        variable_id,
                        region_id,
                        int(row['Year']),
                        float(row['Climate_Value'])
                    ))
                
                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì‚½ì…
                if len(data_to_insert) >= self.batch_size:
                    await conn.executemany("""
                        INSERT INTO climate_data (scenario_id, variable_id, region_id, year, value)
                        VALUES ($1, $2, $3, $4, $5)
                    """, data_to_insert)
                    
                    inserted_count += len(data_to_insert)
                    logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {inserted_count}/{total_rows} ({inserted_count/total_rows*100:.1f}%)")
                    data_to_insert = []
            
            # ë‚¨ì€ ë°ì´í„° ì‚½ì…
            if data_to_insert:
                await conn.executemany("""
                    INSERT INTO climate_data (scenario_id, variable_id, region_id, year, value)
                    VALUES ($1, $2, $3, $4, $5)
                """, data_to_insert)
                inserted_count += len(data_to_insert)
            
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… ë°ì´í„° ì‚½ì… ì™„ë£Œ: {inserted_count}í–‰ ({elapsed_time:.1f}ì´ˆ ì†Œìš”)")
            return True
                
        except Exception as e:
            logger.error(f"âŒ CSV ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
        finally:
            if 'conn' in locals():
                await self.pool.release(conn)
    
    async def close_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.pool:
            await self.pool.close()
            logger.info("ğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    loader = FastClimateDataLoader()
    
    try:
        logger.info("ğŸš€ ë¹ ë¥¸ CSV ë¡œë”© ì‹œì‘...")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        await loader.connect_to_database()
        
        # í…Œì´ë¸” ìƒì„±
        await loader.create_tables()
        
        # ë§ˆìŠ¤í„° ë°ì´í„° ì‚½ì…
        await loader.insert_master_data()
        
        # CSV íŒŒì¼ë“¤ ë¡œë“œ
        csv_files = list(Path(".").glob("*.csv"))
        logger.info(f"ğŸ“‹ ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ")
        
        success_count = 0
        total_start_time = time.time()
        
        for i, csv_file in enumerate(csv_files, 1):
            logger.info(f"ğŸ“ íŒŒì¼ {i}/{len(csv_files)} ì²˜ë¦¬ ì¤‘...")
            if await loader.load_csv_data(csv_file):
                success_count += 1
            else:
                logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {csv_file.name}")
        
        total_elapsed_time = time.time() - total_start_time
        logger.info(f"ğŸ‰ CSV ë¡œë“œ ì™„ë£Œ: {success_count}/{len(csv_files)} ì„±ê³µ")
        logger.info(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_elapsed_time:.1f}ì´ˆ")
        
    except Exception as e:
        logger.error(f"âŒ ë©”ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    finally:
        await loader.close_connection()

if __name__ == "__main__":
    asyncio.run(main())
